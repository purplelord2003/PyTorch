{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMxQoppNwhYoBIwijpQNBhQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## The previous 9 notebooks were learnt using PyTorch 1.x features, in this notebook we summarise the new features introduced in PyTorch 2.x"],"metadata":{"id":"gdUuH0qbfuv5"}},{"cell_type":"markdown","source":["##`torch.compile` introduces various speed-ups to the model training and predictions, especially for transformer based models."],"metadata":{"id":"thgZctUP0s5q"}},{"cell_type":"markdown","source":["## Before 2.0"],"metadata":{"id":"VasIxla4kK5d"}},{"cell_type":"code","source":["import torch\n","import torchvision\n","\n","model = torchvision.models.resnet50()"],"metadata":{"id":"HUKG5iqiiaQU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"wCm_RiU6lDKD","executionInfo":{"status":"ok","timestamp":1706521021423,"user_tz":-480,"elapsed":339,"user":{"displayName":"Kaushik Elangovan","userId":"00639960902829267829"}},"outputId":"2d90175c-d8e6-40dd-f45b-de0b817fc00c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.1.0+cu121'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["## After 2.0"],"metadata":{"id":"H-XmDdqtkO3I"}},{"cell_type":"code","source":["import torch\n","import torchvision\n","\n","model = torchvision.models.resnet50() # could be any model\n","compiled_model = torch.compile(model) # NEW IN 2.0 --> faster model\n","\n"],"metadata":{"id":"ci2jQnjskRgO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The speedups are relative according to how much the GPU is used, greater the number of parameters, greater the speedups. The larger model will take a longer time to train but will be relatively faster. E.g. a model with 1M parameters may take 10 min to train and a model with 10M parameters may take 20 min to train (instead of 100 min)\n","\n","* Can increase batch size\n","* Increase data size (eg. 224x224 instead of 32x32)\n","* Increase model size (use more parameters)\n","* Decreasing data transfer - bandwidth costts (transferring data) will slow down a GPU (it wants to compute on data)\n","\n","As a result of doing the above, the **relative** speedups should be better"],"metadata":{"id":"WH0KiwU34AdZ"}},{"cell_type":"markdown","source":["## Globally set devices + context manager\n","\n","Previously we set the device of our models/tensors with `tensor.to(device)` or `model.to(device)`\n","\n","You can now set the device with a context manager"],"metadata":{"id":"CJhvYsSBluR2"}},{"cell_type":"code","source":["import torch\n","\n","# Set device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Set device with context manager NEW\n","with torch.device(device):\n","  # All tensors or pytorch objects created in this context manager will be on target device\n","  layer = torch.nn.Linear(20, 30)\n","  print(f\"Layer weights are on device: {layer.weight.device}\")\n","  print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")"],"metadata":{"id":"qJHI1qPE_4A_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706541642113,"user_tz":-480,"elapsed":6,"user":{"displayName":"Kaushik Elangovan","userId":"00639960902829267829"}},"outputId":"3cc6020d-197d-40d1-85da-b5fd7a439abe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Layer weights are on device: cpu\n","Layer creating data on device: cpu\n"]}]},{"cell_type":"code","source":["import torch\n","\n","# Set the device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Set device globally NEW\n","torch.set_default_device(device)\n","\n","# All tensors or pytorch objects created will be onn target device\n","layer = torch.nn.Linear(20, 30)\n","print(f\"Layer weights are on device: {layer.weight.device}\")\n","print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q5KWVN8ZzEoG","executionInfo":{"status":"ok","timestamp":1706541647962,"user_tz":-480,"elapsed":305,"user":{"displayName":"Kaushik Elangovan","userId":"00639960902829267829"}},"outputId":"8fa41543-7ed7-44bc-f662-e7200371d227"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Layer weights are on device: cpu\n","Layer creating data on device: cpu\n"]}]},{"cell_type":"markdown","source":["## Other speedups with TF32\n","\n","TF32 = TensorFloat32 a datatype that bridges Float32(FP32 - default in PyTorch) (more accurate) and Float16(FP16) (faster computation)\n","\n","TensorFloat32 is a datatype from NVIDIA which combines the accuracy of float32 and speed of float16\n","\n","Available on NVIDIA Ampere GPUs+\n","\n","Usually works when GPU Score >= (8, 0)\n"],"metadata":{"id":"yMJi5afT9Hck"}},{"cell_type":"code","source":["# Get GPU capability score\n","GPU_SCORE = torch.cuda.get_device_capability()\n","print(f\"GPU capability score: {GPU_SCORE}\")\n","if GPU_SCORE >= (8, 0):\n","  torch.backends.cuda.matmul.allow_tf32 = True\n","else:\n","  torch.backends.cuda.matmul.allow_tf32 = False # if get inconsistent results can change back to false"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":399},"id":"YR_okV10-q5g","executionInfo":{"status":"error","timestamp":1706544566447,"user_tz":-480,"elapsed":4,"user":{"displayName":"Kaushik Elangovan","userId":"00639960902829267829"}},"outputId":"dde87fc3-737d-44ea-c34c-9fc6dc058a51"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-0dfd1e18704d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get GPU capability score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mGPU_SCORE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_capability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"GPU capability score: {GPU_SCORE}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mGPU_SCORE\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_tf32\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_capability\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmajor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mminor\u001b[0m \u001b[0mcuda\u001b[0m \u001b[0mcapability\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \"\"\"\n\u001b[0;32m--> 435\u001b[0;31m     \u001b[0mprop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \"\"\"\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will define _get_device_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"]}]},{"cell_type":"code","source":[],"metadata":{"id":"yv2fIC29-2iB"},"execution_count":null,"outputs":[]}]}